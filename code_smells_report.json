{
    "python": {
        "app.py": {
            "smells": [
                "High Complexity Functions (>10): 1 - ['excelColnoToColNo(11)']",
                "Deeply Nested Functions (>3): 3 - ['refactor_special_missing_values_endpoint(4)', 'refactor_missing_values_endpoint(4)', 'excelColnoToColNo(5)']",
                "Feature Envy (>5 external calls): 1 - ['regularExp(7)']",
                "Data Clumps (Repeated params): 1 sets - [[]]",
                "Shotgun Surgery (Function called >10 times): 2 - [('len', 13), ('jsonify', 15)]",
                "Too Many Returns (>3): 1 - ['excelColnoToColNo(7)']",
                "Global Variables (Not in function/class): 50 - ['len', 'generate_boxplot', 'generate_bargraph_missing_values', 'generate_bargraph_class_imbal', 'refactor_trailing_spaces', 'Response', 'refactor_binary_missing_values', 'i', 'class_imbal', 'duplicated', 'type', 'refactor_binning_cat', '__name__', 'refactor_class_imbal', 'ord', 'refactor_outliers', 'generate_bargraph_human_friendly', 'detect_special_characters', 'human_friendly', 'generate_heatmap', 'refactor_human_friendly', 'str', 'missing_values', 'generate_bargraph_special_missing_values', 'SpecialMissingValues', 'trailing_spaces', 'binning_cat', 'list', 'CORS', 'Flask', 'generate_bargraph_special_characters', 'int', 'jsonify', 'correlated', 'Outliers', 'detect_binary_missing_values', 'col', 'request', 'detect_unique_values', 'refactor_special_char', 'range', 'refactor_unique_values', 'generate_bargraph_binning_cat', 'generate_bargraph_nan_values', 'print', 'detect_integer_as_string', 'generate_bargraph_trailing_spaces', 'refactor_integer_as_string', 'pd', 'excelColnoToColNo']"
            ],
            "metrics": {
                "total_lines": 280,
                "num_functions": 16,
                "function_lengths": [
                    41,
                    7,
                    2,
                    7,
                    4,
                    4,
                    4,
                    4,
                    4,
                    4,
                    4,
                    4,
                    4,
                    4,
                    4,
                    8
                ],
                "num_prints": 10
            },
            "code": "# @Date: 2023-02-28\n# @Title: The core backend Flask code that runs the server, communicates with the frontend using flask_cors, and calls the algorithms.\n\n# pip install numpy panda seaborn matplotlib flask flask_cors\nfrom algorithm.sp_missingvalues import *\nfrom algorithm.correlated import *\nfrom algorithm.outliers import *\nfrom algorithm.duplicates import *\nfrom algorithm.imbalance import *\nfrom algorithm.stringsmells import *\nimport json\n\n# Datasets/concrete.csv\ndf = pd.read_csv('Datasets/concrete.csv')\n\nfrom flask import Flask, request, jsonify, send_file, Response\nfrom flask_cors import CORS\nimport pandas as pd, numpy as np, io, base64, matplotlib.pyplot as plt, seaborn as sns\n\n# Initialize the Flask application\napp = Flask(__name__)\nCORS(app, resources={r\"/*\": {\"origins\": \"http://localhost:3000\"}})\nCORS(app, resources={r\"/*\": {\"origins\": \"http://localhost:3001\"}})\nresults = {}\n\n# route http posts to this method\n@app.route('/upload', methods=['POST'])\ndef upload():\n    file = request.files['file']\n    global df\n    global results\n    df = pd.read_csv(file)\n    print(df)\n    results['num_rows'] = len(df)\n    results['num_cols'] = len(df.columns)\n    results['column_names'] = list(df.columns)\n    results['duplicates'], ds = duplicated(df)\n  \n    spd = SpecialMissingValues(df)\n    results['sp_missing_values'] = {'Info': spd[0], 'InfoNan': spd[1], 'Code': spd[2], 'Code_Nan':spd[3],'splmissCols':spd[4],'missingPer':spd[5]}\n    spd = missing_values(df)\n    results['missing_values'] = {'Info': spd[0],  'Code': spd[1],'missCols':spd[2], 'missPer':spd[3]}\n    results['heatmap'] = generate_heatmap(df)\n    results['correlated'] = correlated(df)\n    results['bargraph_miss'] = generate_bargraph_missing_values(df)\n    results['bargraph_sp_miss'] = generate_bargraph_special_missing_values(df)\n    results['bargraph_nan'] = generate_bargraph_nan_values(df)\n    print(df)\n    bnc = binning_cat(df)\n    results['binning_cat'] = {'Info': bnc[0],  'Code': bnc[1], 'binCols': bnc[2], 'unqVals':bnc[3], 'plot': generate_bargraph_binning_cat(df)}\n    imb = class_imbal(df)\n    results['imbalance'] = {'Info': imb[0] + imb[1], 'imbCols': imb[2], 'imbRatio':imb[3],  'plot': generate_bargraph_class_imbal(df)}\n    # Trailng Spaces, Special Characters, Human Friendly\n    spcr = detect_special_characters(df)\n    results['sp_char'] = {'Info': spcr[0], 'Code': spcr[1], 'plot': generate_bargraph_special_characters(df)}\n    ints=detect_integer_as_string(df)\n    results['int_to_str']= {'Info': ints[0], 'Code': ints[1]}\n    unq = detect_unique_values(df)\n    results['unique_values'] = {'Info': unq[0], 'Code': unq[1]}\n    mis = detect_binary_missing_values(df)\n    results['binary_missing_values'] = {'Info': mis[0], 'Code': mis[1]}\n    trsp = trailing_spaces(df)\n    results['tr_spaces'] = {'Info': trsp[0], 'Code': trsp[1], 'plot': generate_bargraph_trailing_spaces(df)}\n    humf = human_friendly(df)\n    results['hum_friendly'] = {'Info': humf[0], 'Code': humf[1], 'plot': generate_bargraph_human_friendly(df)}\n    outl = Outliers(df)\n    results['outliers'] = {'Info': outl[0], 'Suggestion': outl[1], 'Code': outl[2], 'plot': generate_boxplot(df)}\n    # print(generate_boxplot(df))\n    j = jsonify(results)\n    print(\"-------------------------------------------\")\n    print(j)    \n    return j\n\n\n@app.route('/refactor/special-missing-values', methods=['POST'])\ndef refactor_special_missing_values_endpoint():\n    global df\n    global results\n    spd = SpecialMissingValues(df)\n    results['sp_missing_values'] = {'Info': spd[0], 'InfoNan': spd[1], 'Code': spd[2], 'Code_Nan':spd[3],'splmissCols':spd[4],'missingPer':spd[5]}\n    for col in spd[4]:\n        if df[col].dtype in ['float64', 'int64']:\n            df[col].fillna(df[col].mean(), inplace=True)\n        elif df[col].dtype == 'object':\n            most_frequent = df[col].mode().iloc[0]\n            df[col].fillna(most_frequent, inplace=True)\n    print(df)\n    return jsonify({\"message\": \"Special missing values have been refactored\", \"data\": df.to_dict(orient=\"records\")}), 200\n\n@app.route('/download-dataset', methods=['GET'])\ndef download_dataset():\n    global df\n    # Ensure df is already refactored or processed if needed\n    if df is not None:\n        # Convert the DataFrame to CSV format\n        csv = df.to_csv(index=False)\n        return Response(\n            csv,\n            mimetype=\"text/csv\",\n            headers={\"Content-Disposition\": \"attachment;filename=refactored_dataset.csv\"}\n        )\n    else:\n        return jsonify({\"message\": \"Dataset not found or not refactored yet.\"}), 400\n\n@app.route('/refactor/missing-values', methods=['POST'])\ndef refactor_missing_values_endpoint():\n    global df\n    global results\n\n    spd = missing_values(df)\n    results['missing_values'] = {'Info': spd[0],'Code': spd[1],'missCols': spd[2],'missPer': spd[3]}\n    \n    for col in spd[2]:\n        if df[col].dtype in ['float64', 'int64']:\n            df[col].fillna(df[col].mean(), inplace=True)\n        elif df[col].dtype == 'object':\n            most_frequent = df[col].mode().iloc[0]\n            df[col].fillna(most_frequent, inplace=True)\n    \n    print(df)\n    return jsonify({\"message\": \"Missing values have been refactored\", \"data\": df.to_dict(orient=\"records\")}), 200\n\n\n@app.route('/refactor/duplicate-values', methods=['POST'])\ndef refactor_duplicate_values_endpoint():\n    global df\n    global results\n    results['duplicates'], df = duplicated(df)\n    return jsonify({\"message\": \"Duplicate values have been refactored\", \"data\": df.to_dict(orient=\"records\")}), 200\n@app.route('/refactor/int-to-str', methods=['POST'])\ndef refactor_int_as_str_endpoint():\n    global df\n    global results\n    df=refactor_integer_as_string(df)\n    return jsonify({\"message\": \"Int as String values have been refactored\", \"data\": df.to_dict(orient=\"records\")}), 200\n\n@app.route('/refactor/binning-categorical', methods=['POST'])\ndef refactor_binning_categorical_endpoint():\n    global df\n    global results\n    df=refactor_binning_cat(df)\n    return jsonify({\"message\": \"Binning categorical values have been refactored\", \"data\": df.to_dict(orient=\"records\")}), 200\n\n@app.route('/refactor/class-imbalance', methods=['POST'])\ndef refactor_class_imbalance_endpoint():\n    global df\n    global results\n    df = refactor_class_imbal(df)\n    return jsonify({\"message\": \"Class imbalance has been refactored\", \"data\": df.to_dict(orient=\"records\")}), 200\n\n@app.route('/refactor/special-characters', methods=['POST'])\ndef refactor_special_characters_endpoint():\n    global df\n    global results\n    df=refactor_special_char(df)\n    return jsonify({\"message\": \"Special Characters has been refactored\", \"data\": df.to_dict(orient=\"records\")}), 200\n\n@app.route('/refactor/human-friendly', methods=['POST'])\ndef refactor_human_friendly_endpoint():\n    global df\n    global results\n    df=refactor_human_friendly(df)\n    return jsonify({\"message\": \"Human friendly has been refactored\", \"data\": df.to_dict(orient=\"records\")}), 200\n\n@app.route('/refactor/trailing-spaces', methods=['POST'])\ndef refactor_trailing_spaces_endpoint():\n    global df\n    global results\n    df=refactor_trailing_spaces(df)\n    return jsonify({\"message\": \"Ttrailing Spaces has been refactored\", \"data\": df.to_dict(orient=\"records\")}), 200\n\n@app.route('/refactor/outliers', methods=['POST'])\ndef refactor_outliers_endpoint():\n    global df\n    global results\n    df=refactor_outliers(df)\n    return jsonify({\"message\": \"Outliers has been refactored\", \"data\": df.to_dict(orient=\"records\")}), 200\n\n@app.route('/refactor/unique_values', methods=['POST'])\ndef refactor_unique_values_endpoint():\n    global df\n    global results\n    df=refactor_unique_values(df)\n    return jsonify({\"message\": \"unique-values has been refactored\", \"data\": df.to_dict(orient=\"records\")}), 200\n\n@app.route('/refactor/binary-missing-values', methods=['POST'])\ndef refactor_binary_missing_values_endpoint():\n    global df\n    global results\n    df=refactor_binary_missing_values(df)\n    return jsonify({\"message\": \"binary-missing-values has been refactored\", \"data\": df.to_dict(orient=\"records\")}), 200\n\n# @Use: Converts Excel Column Number to Column Name\ndef excelColnoToColNo(cn:str) :\n    if type(cn)== int or cn.isdigit():\n        cn = int(cn)\n        if cn < 1:\n            return 1\n        return cn\n    cn = cn.upper()\n    for i in range(len(cn)):\n        if not (ord(cn[i]) >= 65 and ord(cn[i]) <= 90):\n            return -1\n    if len(cn) == 1:\n        # A->1\n        return ord(cn) - 64\n    elif len(cn) == 2:\n        # AA->27\n        return (ord(cn[0]) - 64) * 26 + (ord(cn[1]) - 64)\n    \n    elif len(cn) == 3:\n        # AAA->703\n        return (ord(cn[0]) - 64) * 26 * 26 + (ord(cn[1]) - 64) * 26 + (ord(cn[2]) - 64)\n    else:\n        ans = 0\n        for i in range(len(cn)):\n            ans += (ord(cn[i]) - 64) * 26 ** (len(cn) - i - 1)\n        return ans\n    \n@app.route('/regularExp', methods=['POST'])\n# @Use: Custom Data Smell Detection\ndef regularExp():\n    global df\n    #print(df)\n    count = 0\n    # initialize prefetching,\n    colNo = 1\n    regex = request.json['regex']\n    colNo = request.json['colNo']\n\n\n    colNo = excelColnoToColNo(colNo)\n    #string to regex string\n    #filtered_df = df[int(colNo)].str.match[(regex)]\n    #print(filtered_df)\n    # matches = df[col].astype(str).str.match(pattern)\n    # Verify that the column is of type object, and colNo is an integer less than the number of columns\n    # if df[df.columns[int(colNo)]].dtype == 'object' and int(colNo) < len(df.columns):\n    if 0 < int(colNo) < len(df.columns)+1:\n            \n        matches  = df[df.columns[int(colNo)-1]].astype(str).str.match(regex)\n        # print lentgh of matches\n        #count where matches is true\n        count = len(matches[matches == True].index)\n        print(count, len(matches))\n        percent__ = (count/len(matches)).__round__(5)*100\n        colName = df.columns[int(colNo)-1]\n        percent_ ='Column Name : '+ colName + f'\\nPercentage of matches: {percent__}%'\n        \n        #print(matches)\n        print(regex)\n        print(colNo)\n        print(count)\n    else:\n        count = 0\n        percent__ = 0\n        percent_ = f'Please enter a valid column number. It should be less than {len(df.columns)+1} and more than 0.\\n Indexing starts from 1.'\n    \n    return jsonify({'count': count, 'percent': percent_})\nif __name__ == '__main__':\n    app.run(debug= True)\n\n'''\n# define the regular expression to match\nregex_pattern = r'^[a-z]{3}[0-9]{2}$'\n\ncol_name = 'Custom Name'\n# # filter the DataFrame using the regular expression\nfiltered_df = df[col_name].str.match(regex_pattern)]\n\n# for col in filtered_df.columns:\n#     filtered_df = filtered_df[filtered_df[col].str.match(regex_pattern)]\n\n# print the filtered DataFrame\nprint(filtered_df)\n\n# print the filtered DataFrame\nprint(filtered_df)\n\n'''"
        },
        "correlated.py": {
            "smells": [
                "Deeply Nested Functions (>3): 1 - ['correlated(4)']",
                "Feature Envy (>5 external calls): 2 - ['generate_heatmap(18)', 'correlated(17)']",
                "Data Clumps (Repeated params): 1 sets - [['df']]",
                "Global Variables (Not in function/class): 13 - ['len', 'np', 'matplotlib', 'io', 'range', 'j', 'df', 'plt', 'set', 'base64', 'i', 'str', 'abs']"
            ],
            "metrics": {
                "total_lines": 69,
                "num_functions": 2,
                "function_lengths": [
                    13,
                    10
                ],
                "num_prints": 0
            },
            "code": "import matplotlib\nmatplotlib.use('Agg')  \n\nimport pandas as pd\nimport numpy as np\nimport io\nimport base64\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef generate_heatmap(df):\n    # Select only numeric columns\n    df_numeric = df.select_dtypes(include=[np.number])\n    # Compute the correlation matrix\n    corr = df_numeric.corr()\n\n    if corr.empty:\n        return None\n    \n    plt.imshow(corr, cmap='coolwarm', interpolation='none')\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.colorbar()\n\n    # Save the plot to a BytesIO object\n    img_bytes = io.BytesIO()\n    plt.savefig(img_bytes, format='png')\n    img_bytes.seek(0)\n\n    # Encode the image data as base64 string\n    img_base64 = base64.b64encode(img_bytes.read()).decode('utf-8')\n    plt.close()\n    \n    return img_base64\n\n\ndef correlated(df):\n    # Select only numeric columns\n    df_numeric = df.select_dtypes(include=[np.number])\n    # Compute the correlation matrix\n    corr_matrix = df_numeric.corr()\n    \n    instr = []\n    if corr_matrix.empty:\n        instr.append(\"There are no highly correlated features in the dataset.\\n\")\n        return instr\n\n    # Identify highly correlated features\n    high_corr_features = set()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > 0.7:  # Change threshold value as needed\n                colname = corr_matrix.columns[i]\n                high_corr_features.add(colname)\n\n    # Maximum correlation value among any two values\n    max_corr = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_)).stack().max()\n    instr.append(\"Maximum correlation value among any two distinct values: \" + str(max_corr) + \"\\n\")\n\n    if len(high_corr_features) > 0:\n        instr.extend([\n            \"There are highly correlated features in the dataset.\\n\",\n            \"Number of highly correlated features: \" + str(len(high_corr_features)) + \"\\n\",\n            \"Highly correlated features: \" + str(high_corr_features) + \"\\n\"\n        ])\n    else:\n        instr.append(\"There are no highly correlated features in the dataset.\\n\")\n\n    return instr\n"
        },
        "duplicates.py": {
            "smells": [
                "Feature Envy (>5 external calls): 1 - ['duplicated(6)']",
                "Global Variables (Not in function/class): 6 - ['len', 'df', 'any', 'min', 'round', 'str']"
            ],
            "metrics": {
                "total_lines": 22,
                "num_functions": 1,
                "function_lengths": [
                    4
                ],
                "num_prints": 0
            },
            "code": "def duplicated(df):\n    duplicates = df.duplicated()\n    instr = ''\n    if any(duplicates):\n        # Print information about duplicates\n        instr += \"Duplicate examples are present in the dataset.\\n\"\n        instr += \"Number of duplicate examples: \" + str(duplicates.sum()) + \"\\n\"\n        instr += f\"Percentage of duplicate examples: {round(duplicates.sum() / df.shape[0] * 100, 3)} %\\n\"\n        dl = df.index[duplicates].tolist()\n        instr += \"Indices of duplicate examples: \" + str(dl[: min(len(dl), 30)]) + \"\\n\"\n        if len(dl) > 30:\n            instr += \"(Only the first 30 duplicate examples are shown.)\\n\"\n        \n        # Removing duplicates\n        df_cleaned = df.drop_duplicates()\n        instr += \"\\nDuplicates have been removed from the dataset.\"\n        \n    else:\n        instr += \"There are no duplicate examples in the dataset.\\n\"\n        df_cleaned = df\n\n    return instr, df_cleaned\n"
        },
        "imbalance.py": {
            "smells": [
                "High Complexity Functions (>10): 1 - ['generate_bargraph_class_imbal(13)']",
                "Deeply Nested Functions (>3): 3 - ['binning_cat(4)', 'refactor_binning_cat(4)', 'class_imbal(4)']",
                "Feature Envy (>5 external calls): 4 - ['class_imbal(9)', 'refactor_class_imbal(7)', 'generate_bargraph_class_imbal(12)', 'generate_bargraph_binning_cat(12)']",
                "Data Clumps (Repeated params): 2 sets - [['df'], ['df', 'threshold']]",
                "Shotgun Surgery (Function called >10 times): 1 - [('len', 19)]",
                "Global Variables (Not in function/class): 12 - ['len', 'col', 'np', 'pd', 'io', 'plt', 'print', 'base64', 'round', 'resample', 'str', 'num']"
            ],
            "metrics": {
                "total_lines": 234,
                "num_functions": 6,
                "function_lengths": [
                    8,
                    7,
                    9,
                    3,
                    13,
                    13
                ],
                "num_prints": 2
            },
            "code": "# @Date: 2023-03-06\n# @Title: This program checks for class imbalance and binary categorical columns with high cardinality in the dataset.\n# It also contains the code to generate infographs for the dataset\n\n\nimport numpy as np, matplotlib.pyplot as plt, io, base64\nimport pandas as pd\nfrom sklearn.utils import resample\n\n'''for col in df.columns:\n    # check if the column is categorical\n    if df[col].dtype == 'object':\n        # check the number of unique values\n        unique_vals = len(df[col].unique())\n        # set a threshold for high cardinality\n        threshold = 10\n        if unique_vals > threshold:\n            print(f\"The '{col}' column has high cardinality with {unique_vals} unique values.\")'''\nbin_cat_present = False\n# @Use: To check for binary categorical columns with high cardinality in the dataset\ndef binning_cat(df):\n    present = False\n    binCols = []\n    unqVals = []\n    s = ''; code = '' \n    # for each column, check if the column is categorical\n    for col in df.columns:\n        if df[col].dtype == 'object':\n            # check the number of unique values\n            unique_vals = len(df[col].unique())\n            # set a threshold for high cardinality\n            threshold = 20\n            if unique_vals > threshold:\n                binCols.append(col)\n                unqVals.append(str(unique_vals))\n                present = True; global bin_cat_present; bin_cat_present = True\n                # print(f\"The '{col}' column has high cardinality with {unique_vals} unique values.\")\n                s += f'''The '{col}' column has high cardinality with {unique_vals} unique values.\\n\n                '''\n\n\n                # # create a list of the top 10 most frequent categories\n                # top_10 = df[col].value_counts().sort_values(ascending=False).head(10).index\n                # # replace all other categories with 'Other'\n                # df[col] = np.where(df[col].isin(top_10), df[col], 'Other')\n                # # check if the code worked\n                # print(df[col].value_counts())\n                # print()\n\n    if present == False:\n        s += '''No categorical column with high cardinality'''\n    else:\n        s += f'The following code can be used to bin the categorical columns with high cardinality:'\n        code += f'''\nfor col in df.columns:\n    if df[col].dtype == 'object':\n        unique_vals = len(df[col].unique())\n        threshold = 10\n        if unique_vals > threshold:\n            # create a list of the top 10 most frequent categories\n            top_10 = df[col].value_counts().sort_values(ascending=False).head(10).index\n            # replace all other categories with 'Other'\n            df[col] = np.where(df[col].isin(top_10), df[col], 'Other')\n            # check if the code worked\n            print(df[col].value_counts())\n            print()\\n'''\n\n    return s, code, binCols, unqVals\n\n\ndef refactor_binning_cat(df):\n    present = False\n    binCols = []\n    unqVals = []\n    s = ''; code = '' \n    # for each column, check if the column is categorical\n    for col in df.columns:\n        if df[col].dtype == 'object':\n            unique_vals = len(df[col].unique())\n            threshold = 10\n            if unique_vals > threshold:\n                # create a list of the top 10 most frequent categories\n                top_10 = df[col].value_counts().sort_values(ascending=False).head(10).index\n                # replace all other categories with 'Other'\n                df[col] = np.where(df[col].isin(top_10), df[col], 'Other')\n                print(df[col].value_counts())\n                print()\n    return df\n\n'''Refer this:\nimb = False; nu = 1\n# Check for class imbalance\nfor col in df.columns:\n    class_counts = df[col].value_counts()\n    if class_counts.min() / class_counts.max() < 0.1 and not col in ['Date', 'Time', 'Name' ]:\n            # if all values are unique, then continue\n        if  len(df[col].unique()) == len(df):\n            continue\n        # print(f\"{nu}) Class imbalance detected in column {col} with \", end= '')\n        imb = True        \n        # print(\"Class imbalance ratio:\", round(class_counts.min() / class_counts.max(), 2),'\\n')\n        nu += 1\n        # ck = class_counts.to_dict()\n        # # print few elements of dictionary\n        # print(\"Class counts:\", {k: ck[k] for k in list(ck)[:5]}, '... etc')\n\nif imb:\n    print(\"Potential mitigation strategies:\")\n    print(\"- Use appropriate sampling techniques to balance the classes.\")\n    print(\"- Use appropriate evaluation metrics like F1 score, precision, recall, etc. as accuracy is not a good metric for imbalanced datasets.\")\n    print(\"- Use appropriate regularization techniques like class weights to combat bias.\")\nelse:\n    print(\"There is no class imbalance in the dataset.\")\n    print(\"No mitigation strategies are required.\")\n'''\nclass_imbal_present = False\n# @Use: To check for class imbalance in the dataset\ndef class_imbal(df, threshold = 0.01):\n    global class_imbal_present\n    s = ''; code = ''; nu = 1\n    imbCols = []\n    imbRatio = []\n    # Check for class imbalance\n    for col in df.columns:\n        class_counts = df[col].value_counts()\n        if class_counts.min() / class_counts.max() < threshold and not col in ['Date', 'Time', 'Name' ]:\n                # if all values are unique, then continue\n            if  len(df[col].unique()) == len(df) or len(df[col].unique()) >= 5:\n                continue\n            class_imbal_present = True\n            imbCols.append(col)\n            imbRatio.append(str(round(class_counts.min() / class_counts.max(), 2)))\n            s += f\"{nu}) Class imbalance detected in column {col} with Class imbalance ratio: {round(class_counts.min() / class_counts.max(), 2)}\\n\"\n            nu += 1\n            # ck = class_counts.to_dict()\n            # # print few elements of dictionary\n            # print(\"Class counts:\", {k: ck[k] for k in list(ck)[:5]}, '... etc')\n\n    if class_imbal_present:\n        s += f'''Potential mitigation strategies:\n        - Use appropriate sampling techniques to balance the classes.   \n        - Use appropriate evaluation metrics like F1 score, precision, recall, \\netc. as accuracy is not a good metric for imbalanced datasets.\n        - Use appropriate regularization techniques like class weights to combat bias.\\n'''\n\n    else:\n        s += f'''There is no class imbalance in the dataset.\n        No mitigation strategies are required.\\n'''\n\n    return s, code, imbCols, imbRatio\n\ndef refactor_class_imbal(df, threshold=0.1):\n    imbCols = []\n    \n    for col in df.columns:\n        if  len(df[col].unique()) == len(df) or len(df[col].unique()) >= 5:\n                continue\n        class_counts = df[col].value_counts()\n        if class_counts.min() / class_counts.max() < threshold and len(df[col].unique()) != len(df):\n            imbCols.append(col)\n            # Resampling to handle class imbalance\n            major_class = class_counts.idxmax()\n            minor_class = class_counts.idxmin()\n\n            # Separate the majority and minority classes\n            df_majority = df[df[col] == major_class]\n            df_minority = df[df[col] == minor_class]\n\n            # Upsample the minority class to match the majority class size\n            df_minority_upsampled = resample(df_minority,\n                                             replace=True,  # Sample with replacement\n                                             n_samples=(len(df_majority)-len(df_minority))/10,  # Match the majority class size\n                                             random_state=42)  # For reproducibility\n\n            # Combine the majority class with the upsampled minority class\n            df = pd.concat([df, df_minority_upsampled])\n\n    return df\n\n\n\ndef generate_bargraph_class_imbal(df, threshold = 0.1):\n    # The bar graph show show the ratio of max_count/(min_count + 1) for culprit columns\n\n    pres = {num : 0 for num in df.columns if df[num].dtype != 'object' and not num in ['Date', 'Time', 'Name' ] and len(df[num].unique()) != len(df) and df[num].value_counts().min() / (df[num].value_counts().max() + 1 ) < threshold}\n    # If the graph is empty, return None\n    if not pres:\n        return None\n    global class_imbal_present\n    if class_imbal_present == False:\n        return None\n    \n    for num in df.columns:\n        if df[num].dtype != 'object' and not num in ['Date', 'Time', 'Name' ] and len(df[num].unique()) != len(df) and df[num].value_counts().min() / (df[num].value_counts().max() + 1 ) < threshold:\n            pres[num] = df[num].value_counts().max() / (df[num].value_counts().min() + 1 )\n    plt.bar(pres.keys(), pres.values())\n    plt.xticks(rotation=90)\n    # Save the plot to a BytesIO object\n    img_bytes = io.BytesIO()\n    plt.savefig(img_bytes, format='png')\n    # Seek to the start of the stream\n    img_bytes.seek(0)\n    img_base64 = base64.b64encode(img_bytes.read()).decode('utf-8')\n    plt.close()\n\n    return img_base64\n\n\n# @Use: To generate a bar graph for categorical columns with high cardinality\ndef generate_bargraph_binning_cat(df):\n    # Check for categorical column with high cardinality.\n    # Display a bar graph that tells the number of special missing values in each column of the dataset\n    pres = {num : 0 for num in df.columns if df[num].dtype == 'object' and len(df[num].unique()) > 10}\n    # If the graph is empty, return None\n    if not pres:\n        return None\n    global bin_cat_present\n    if bin_cat_present == False:\n        return None\n    for num in df.columns:\n        if df[num].dtype == 'object' and len(df[num].unique()) > 10:\n            pres[num] = len(df[num].unique())\n    plt.bar(pres.keys(), pres.values())\n    plt.xticks(rotation=45)\n    # Save the plot to a BytesIO object\n    img_bytes = io.BytesIO()\n    plt.savefig(img_bytes, format='png')\n    img_bytes.seek(0)\n    # Encode the image data as base64 string\n    img_base64 = base64.b64encode(img_bytes.read()).decode('utf-8')\n    plt.close()\n\n    return img_base64\n    \n            "
        },
        "outliers.py": {
            "smells": [
                "Deeply Nested Functions (>3): 1 - ['Outliers(4)']",
                "Feature Envy (>5 external calls): 1 - ['generate_boxplot(10)']",
                "Data Clumps (Repeated params): 1 sets - [['df']]",
                "Global Variables (Not in function/class): 9 - ['Exception', 'col', 'np', 'io', 'sns', 'plt', 'base64', 'round', 'str']"
            ],
            "metrics": {
                "total_lines": 108,
                "num_functions": 3,
                "function_lengths": [
                    8,
                    13,
                    1
                ],
                "num_prints": 0
            },
            "code": "# @Date: 2023-03-06\n# @Title: This program checks for outliers in the dataset.\n\nimport pandas as pd, numpy as np, io, base64, matplotlib.pyplot as plt, seaborn as sns\n\n# To generate the boxplot for the outliers\ndef generate_boxplot(df):\n    # sns.boxplot(data=df.select_dtypes(include=['float64', 'int64']))\n    # check if there are columns with the right data type\n    if df.select_dtypes(include=['float64', 'int64']).shape[1] > 0:\n        sns.boxplot(data=df.select_dtypes(include=['float64', 'int64']))\n    else:\n        # sns.boxplot(data=df)\n        return None\n    plt.xticks(rotation=90)\n    # Save the plot to a BytesIO object\n    img_bytes = io.BytesIO()\n    plt.savefig(img_bytes, format='png')\n    img_bytes.seek(0)\n    # Encode the image data as base64 string\n    img_base64 = base64.b64encode(img_bytes.read()).decode('utf-8')\n    plt.close()\n    return img_base64\n\n# To check the outliers in the dataset\ndef Outliers(df):\n    instr = ''\n    # print(\"Outliers:\")\n    instr += str(\"Outliers:\\n\")\n    present = False\n    num_outliers = 0\n    total_size = 0\n    for col in df.columns:\n        if df[col].dtype == np.float64 or df[col].dtype == np.int64:\n            # # print(col)\n            # instr += str(col + \"\\n\")\n            # # print(\"Mean:\", df[col].mean())\n            # instr += str(\"Mean: \" + str(df[col].mean()) + \"\\n\")\n            # # print(\"Standard deviation:\", df[col].std())\n            # instr += str(\"Standard deviation: \" + str(df[col].std()) + \"\\n\")\n            # # print(\"Minimum value:\", df[col].min())\n            # instr += str(\"Minimum value: \" + str(df[col].min()) + \"\\n\")\n            # # print(\"Maximum value:\", df[col].max())\n            # instr += str(\"Maximum value: \" + str(df[col].max()) + \"\\n\")\n            # # print(\"Number of outliers:\", df[(df[col] < df[col].mean() - 3 * df[col].std()) | (df[col] > df[col].mean() + 3 * df[col].std())].shape[0])\n            # instr += str(\"Number of outliers: \" + str(df[(df[col] < df[col].mean() - 3 * df[col].std()) | (df[col] > df[col].mean() + 3 * df[col].std())].shape[0]) + \"\\n\")\n            num_outliers += df[(df[col] < df[col].mean() - 3 * df[col].std()) | (df[col] > df[col].mean() + 3 * df[col].std())].shape[0]\n            # # print(\"Percentage of outliers:\", round(df[(df[col] < df[col].mean() - 3 * df[col].std()) | (df[col] > df[col].mean() + 3 * df[col].std())].shape[0] / df.shape[0] * 100, 2), \"%\")\n            total_size += df.shape[0]\n            # instr += str(\"Percentage of outliers: \" + str(round(df[(df[col] < df[col].mean() - 3 * df[col].std()) | (df[col] > df[col].mean() + 3 * df[col].std())].shape[0] / df.shape[0] * 100, 2)) + \" %\\n\")\n            # # print(\"Indices of outliers:\", df[(df[col] < df[col].mean() - 3 * df[col].std()) | (df[col] > df[col].mean() + 3 * df[col].std())].index.tolist())\n            # ol = df[(df[col] < df[col].mean() - 3 * df[col].std()) | (df[col] > df[col].mean() + 3 * df[col].std())].index.tolist()\n            # instr += str(\"Indices of outliers(first 20): \" + str(ol[:min(20, len(ol))]) + \"\\n\")\n            # # print()\n            # instr += str(\"\\n\")\n            if df[(df[col] < df[col].mean() - 3 * df[col].std()) | (df[col] > df[col].mean() + 3 * df[col].std())].shape[0] > 0:\n                present = True\n    # print(\"Number of outliers:\", num_outliers)\n    instr += str(\"Number of outliers: \" + str(num_outliers) + \"\\n\")\n    # print(\"Percentage of outliers:\", round(num_outliers / total_size * 100, 2), \"%\")\n    if total_size == 0:\n        instr += str(\"Percentage of outliers: 0 %\\n\")\n    else:\n        instr += str(\"Percentage of outliers: \" + str(round(num_outliers / total_size * 100)) + \" %\\n\")\n    sugg = ''; code = ''\n    if not present:\n        # print(\"There are no outliers in the dataset.\")\n        sugg += str(\"There are no outliers in the dataset.\\n\")\n\n    else:\n        # print(\"There are outliers in the dataset.\")\n        sugg += str(\"There are outliers in the dataset.\\n\")\n        # # statistics:\n        # sugg += f'Number of outliers: {df[(df[col] < df[col].mean() - 3 * df[col].std()) | (df[col] > df[col].mean() + 3 * df[col].std())].shape[0]}\\n'\n        # sugg += f'Percentage of outliers: {round(df[(df[col] < df[col].mean() - 3 * df[col].std()) | (df[col] > df[col].mean() + 3 * df[col].std())].shape[0] / df.shape[0] * 100)}%\\n'\n        # # print(\"You can remove the outliers from the dataset using the following code:\")\n        sugg += str(\"You can remove the outliers from the dataset using the following code:\\n\")\n        code += '''# Remove outliers\ninitial_shape = df.shape\nfor col in df.columns:\n    if df[col].dtype == np.float64 or df[col].dtype == np.int64:\n        df = df[(df[col] >= df[col].mean() - 3 * df[col].std()) \n        & (df[col] <= df[col].mean() + 3 * df[col].std())]\n        # print(\"Number of outliers removed from\", col + \":\", initial_shape[0] - df.shape[0])\n        # print(\"Percentage of outliers removed from\", col + \":\",\n        # round((initial_shape[0] - df.shape[0]) / initial_shape[0] * 100, 2), \"%\")\n\n            '''\n        \n    instr += sugg\n    return instr, sugg, code\n\n\n\ndef refactor_outliers(df):\n    try:\n        # Loop through each column to detect and remove outliers for numerical columns\n        for col in df.columns:\n            if df[col].dtype == np.float64 or df[col].dtype == np.int64:\n                # Calculate the mean and standard deviation for outlier detection\n                mean = df[col].mean()\n                std_dev = df[col].std()\n                # Remove the outliers based on the 3-sigma rule\n                df = df[(df[col] >= mean - 3 * std_dev) & (df[col] <= mean + 3 * std_dev)]\n        return df\n    except Exception as e:\n        # In case of any exception, return the original dataframe\n        return df\n"
        },
        "sp_missingvalues.py": {
            "smells": [
                "High Complexity Functions (>10): 1 - ['SpecialMissingValues(11)']",
                "Deeply Nested Functions (>3): 2 - ['SpecialMissingValues(4)', 'missing_values(5)']",
                "Feature Envy (>5 external calls): 4 - ['generate_bargraph_special_missing_values(12)', 'generate_bargraph_nan_values(12)', 'SpecialMissingValues(15)', 'missing_values(6)']",
                "Data Clumps (Repeated params): 1 sets - [['df']]",
                "Global Variables (Not in function/class): 13 - ['col', 'sum', 'matplotlib', 'val', 'io', 'df', 'generate_bargraph_nan_values', 'any', 'plt', 'base64', 'round', 'str', 'num']"
            ],
            "metrics": {
                "total_lines": 183,
                "num_functions": 5,
                "function_lengths": [
                    12,
                    11,
                    12,
                    8,
                    1
                ],
                "num_prints": 0
            },
            "code": "# @Date: 2023-02-26\n# @Description: This program checks for missing values, special missing values and generates a bargraph for the number of special missing values in each feature.\nimport matplotlib\nmatplotlib.use('Agg')  \n\nimport pandas as pd, numpy as np, io, base64, matplotlib.pyplot as plt, seaborn as sns\n\n# Check for special missing value.\n# Display a bar graph that tells the number of special missing values in each column of the dataset\ndef generate_bargraph_special_missing_values(df):\n    special_missing_values = ['-', 'n/a', 'N/A', 'NA', '--', '?']\n    pres = {num : 0 for num in df.columns if df[num].isin(special_missing_values).sum()}\n    # If the graph is empty, return None\n    if not pres:\n        return None\n    for num in df.columns:\n        if df[num].isin(special_missing_values).sum():\n            pres[num] = df[num].isin(special_missing_values).sum()\n    plt.bar(pres.keys(), pres.values())\n    plt.xticks(rotation=90)\n    # Save the plot to a BytesIO object\n    img_bytes = io.BytesIO()\n    plt.savefig(img_bytes, format='png')\n    img_bytes.seek(0)\n    # Encode the image data as base64 string\n    img_base64 = base64.b64encode(img_bytes.read()).decode('utf-8')\n    plt.close()\n\n    return img_base64\n\n# Check for NaN values\n# Display a bar graph that tells the number of NaN values in each column of the dataset\ndef generate_bargraph_nan_values(df):\n    pres = {num : 0 for num in df.columns if df[num].isnull().sum() >0}\n    # If the graph is empty, return None\n    if not pres:\n        return None\n    for num in df.columns:\n        if df[num].isnull().sum():\n            pres[num] = df[num].isnull().sum()\n    plt.bar(pres.keys(), pres.values())\n    plt.xticks(rotation=90)\n    # Save the plot to a BytesIO object\n    img_bytes = io.BytesIO()\n    plt.savefig(img_bytes, format='png')\n    img_bytes.seek(0)\n    # Encode the image data as base64 string\n    img_base64 = base64.b64encode(img_bytes.read()).decode('utf-8')\n    plt.close()\n    return img_base64\n\n\n# Check for special missing values\ndef SpecialMissingValues(df):\n    s = ''; s2 = ''\n    special_missing_values = ['-', 'n/a', 'N/A', 'NA', '--', '?']\n    pres = {val: 0 for val in special_missing_values}\n    for val in special_missing_values:\n        pres[val] = df.isin([val]).sum().sum()\n    sugg = ''\n    splmissCols = []\n    missingPer = []\n    if any(pres.values()):\n    #     print(\"There are special missing values in the dataset.\")\n    #     print(\"Number of special missing values:\", sum(pres.values()))\n    #     print(\"Percentage of special missing values:\", round(sum(pres.values()) / (df.shape[0] * df.shape[1]) * 100, 2), \"%\")\n    #     print(\"Special missing values in each column:\")\n    #     for col in df.columns:\n    #         print(col, \":\", df[col].isin(special_missing_values).sum())\n    # else:\n    #     print(\"There are no special missing values in the dataset.\")\n        s += f'''There are special missing values in the dataset.\\n\n    Number of special missing values: {sum(pres.values())}\\n\n    Percentage of special missing values: {round(sum(pres.values()) / (df.shape[0] * df.shape[1]) * 100, 2)} %\\n\n    Special missing values in each column:\\n\n    '''\n        for col in df.columns:\n            if df[col].isin(special_missing_values).sum():\n                splmissCols.append(col)\n                missingPer.append(str(df[col].isin(special_missing_values).sum()))\n                s += f'''{col}: {df[col].isin(special_missing_values).sum()}\\n'''\n    \n    # Refactorting :\n        sugg + '# for each column, replace the special missing values with NaN values.\\n'\n        sugg += 'for col in df.columns:\\n'\n        sugg += '    df[col] = df[col].replace(special_missing_values, np.nan)\\n'\n        # sugg += '# check for NaN values again\\n'\n        sugg += 'df.isnull().sum().sum()\\n'\n\n        # sugg += '# for each column, replace the NaN values with the mean of the column if the column is numeric.\\n'\n        sugg += 'for col in df.columns:\\n'\n        sugg += '    if df[col].dtype == np.float64 or df[col].dtype == np.int64:\\n'\n        sugg += '        df[col] = df[col].fillna(df[col].mean())\\n'\n        # sugg += '# check for NaN values again\\n'\n        sugg += 'df.isnull().sum().sum()\\n'\n\n        # sugg += '# for each column, replace the NaN values with the mode of the column if the column is categorical.\\n'\n        sugg += 'for col in df.columns:\\n'\n        sugg += '    if df[col].dtype == np.object:\\n'\n        sugg += '        df[col] = df[col].fillna(df[col].mode()[0])\\n'\n        # sugg += '# check for NaN values again\\n'\n        \n        # s += sugg\n    else:\n        s += \"There are no special missing values in the dataset.\\n\"\n\n    # Check for NaN values\n    sugg_2 = ''\n    if df.isnull().values.any():\n        s2 += f'''There are NaN values in the dataset.\\n\n    Number of NaN values: {df.isnull().sum().sum()}\\n\n    Percentage of NaN values: {round(df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100, 2)} %\\n\n    NaN values in each column:\\n\n    '''\n        lim = 10\n        for col in df.columns:\n            if df[col].isnull().sum() and lim:\n                s2 += f'''{col}: {df[col].isnull().sum()}\\n'''\n                lim -= 1  \n        if lim == 0:\n            s2 += 'and few more...\\n'\n    # Refactoring :\n        sugg_2 += '# for each column, replace the NaN values with the mean of the column if the column is numeric.\\n'\n        \n        sugg_2 += 'for col in df.columns:\\n'\n        sugg_2 += '    if df[col].dtype == np.float64 or df[col].dtype == np.int64:\\n'\n        sugg_2 += '        df[col] = df[col].fillna(df[col].mean())\\n'\n        sugg_2 += '    elif df[col].dtype == np.object:\\n'\n        sugg_2 += '        df[col] = df[col].fillna(df[col].mode()[0])\\n'\n        sugg_2 += '# check for NaN values again\\n'\n        sugg_2 += 'df.isnull().sum().sum()\\n'\n        # s += sugg_2\n\n        \n\n    else:\n        s2 += \"\\nThere are no NaN values in the dataset.\"\n\n    return s,s2,sugg,sugg_2,splmissCols,missingPer\n\n\n# Check for missing values\ndef missing_values(df):\n    v = df.isnull().sum().sum()\n    s = ''; sugg = ''; code = ''\n    missCols = []\n    missPer = []\n    if not v:\n        s += \"There are no missing values in the dataset.\\n\"\n    else:\n        s += f'''There are missing values in the dataset.\\n\n    Number of missing values: {v}\\n\n    Percentage of missing values: {round(v / (df.shape[0] * df.shape[1]) * 100, 2)} %\\n\n    Missing values in each column:\\n\n    '''\n        limit = 10\n        for col in df.columns:\n            if df[col].isnull().sum():\n                missCols.append(col)\n                missPer.append(str(df[col].isnull().sum()))\n                s += f'''{col}: {df[col].isnull().sum()}\\n'''\n                if not limit:\n                    break\n                limit -= 1\n        code += '# for each column, replace the missing values with the mean of the column if the column is numeric.\\n'\n        code += 'for col in df.columns:\\n'\n        code += '    if df[col].dtype == np.float64 or df[col].dtype == np.int64:\\n'\n        code += '        df[col] = df[col].fillna(df[col].mean())\\n'\n        code += '# check for missing values again\\n'\n        code += 'df.isnull().sum().sum()\\n'\n        code += '# for each column, replace the missing values with the mode of the column if the column is categorical.\\n'\n        code += 'for col in df.columns:\\n'\n        code += '    if df[col].dtype == np.object:\\n'\n        code += '        df[col] = df[col].fillna(df[col].mode()[0])\\n'\n        code += '# check for missing values again\\n'\n        code += 'df.isnull().sum().sum()\\n'\n\n    return s, code, missCols, missPer\n\ndef generate_bargraph_missing_values(df):    \n    return generate_bargraph_nan_values(df)\n\n     "
        },
        "stringsmells.py": {
            "smells": [
                "Deeply Nested Functions (>3): 8 - ['detect_binary_missing_values(4)', 'detect_integer_as_string(5)', 'detect_special_characters(4)', 'refactor_special_char(4)', 'refactor_integer_as_string(6)', 'trailing_spaces(4)', 'human_friendly(4)', 'refactor_human_friendly(4)']",
                "Feature Envy (>5 external calls): 3 - ['generate_bargraph_special_characters(12)', 'generate_bargraph_trailing_spaces(12)', 'generate_bargraph_human_friendly(12)']",
                "Data Clumps (Repeated params): 2 sets - [['df'], ['cell']]",
                "Shotgun Surgery (Function called >10 times): 1 - [('len', 12)]",
                "Too Many Returns (>3): 2 - ['detect_integer_as_string(4)', 'refactor_integer_as_string(4)']",
                "Global Variables (Not in function/class): 19 - ['Exception', 'len', 'convert_cell', 'check_cell', 'type', 'isinstance', 'plt', 'str', 'cell', 'int', 'io', 'e', 'bool', 'float', 'base64', 'num', 'col', 're', 'print']"
            ],
            "metrics": {
                "total_lines": 399,
                "num_functions": 17,
                "function_lengths": [
                    5,
                    2,
                    4,
                    1,
                    4,
                    5,
                    2,
                    1,
                    1,
                    4,
                    1,
                    3,
                    7,
                    1,
                    12,
                    2,
                    2
                ],
                "num_prints": 7
            },
            "code": "# @Date: 2020-09-27\n# @Description: This program checks for string smells in the dataset.\n\nimport re, matplotlib.pyplot as plt, io, base64, numpy as np, pandas as pd\n\ninteger_as_string_present = False\ndef detect_binary_missing_values(df):\n    \"\"\"\n    Detect columns with mostly missing values, but where all non-missing values are 'Y'.\n    This implies that missing values might represent 'N' vs truly missing data.\n    \"\"\"\n    binary_missing_features = []\n    code = ''\n    s = ''\n    try:\n        for col in df.columns:\n            # Calculate fraction of missing values\n            fraction_missing = df[col].isna().mean()\n            # Check if at least 90% are missing\n            if fraction_missing >= 0.9:\n                non_missing_vals = df[col].dropna().unique()\n                # If all non-missing values are 'Y', consider this a binary missing column\n                if len(non_missing_vals) == 1 and non_missing_vals[0] == 'Y':\n                    binary_missing_features.append(col)\n\n        if binary_missing_features:\n            s = \"There are columns with mostly missing values, but the non-missing values are 'Y'.\\n\"\n            s += \"Affected columns: \" + str(binary_missing_features[:8])\n            if len(binary_missing_features) > 8:\n                s += \"...\\n\"\n            else:\n                s += \"\\n\"\n            s += \"These missing values may implicitly indicate 'N'.\\n\"\n\n            code += f'''\n# Example refactoring: Convert missing values to 'N' for columns identified\nbinary_missing_columns = {binary_missing_features}\nfor col in binary_missing_columns:\n    df[col] = df[col].fillna('N')  # Treat missing as 'N'\n'''\n        else:\n            s = \"No columns with binary missing values detected.\\n\"\n        return s, code\n\n    except Exception as e:\n        return f\"Error in detect_binary_missing_values: {{str(e)}}\\\\n\", \"\"\n\ndef refactor_binary_missing_values(df):\n    \"\"\"\n    Refactor columns with binary missing values by filling missing entries with 'N'.\n    \"\"\"\n    try:\n        for col in df.columns:\n            df[col] = df[col].fillna('N')  # Treat missing as 'N'\n        return df\n    except Exception as e:\n        print(f\"Error in refactor_binary_missing_values: {str(e)}\")\n        return df\ndef detect_unique_values(df):\n    unique_identifier_features = []\n    code = ''\n    s = ''\n    try:\n        for col in df.columns:\n            # Check if column is a unique identifier (all rows distinct)\n            if df[col].nunique() == len(df):\n                unique_identifier_features.append(col)\n\n        if unique_identifier_features:\n            s = \"There are columns containing unique identifiers (uids) in the dataset.\\n\"\n            s += \"Affected columns: \" + str(unique_identifier_features[:8])\n            if len(unique_identifier_features) > 8:\n                s += \"...\\n\"\n            else:\n                s += \"\\n\"\n            s += \"These columns appear to hold a primary key or unique ID.\\n\"\n\n            code += f'''\n# Example refactoring: drop or manage unique identifier columns\nuid_columns = {unique_identifier_features}\ndf = df.drop(columns=uid_columns)  # if not needed\n'''\n        else:\n            s = \"No unique identifier columns detected.\\n\"\n        return s, code\n\n    except Exception as e:\n        return f\"Error in detect_unique_values: {str(e)}\\n\", \"\"\n\ndef refactor_unique_values(df):\n    try:\n        for col in df.columns:\n            if df[col].nunique() == len(df):\n                df = df.drop(columns=[col])\n        return df\n    except Exception as e:\n        print(f\"Error in refactor_unique_values: {str(e)}\")\n        return df\n\n\ndef detect_integer_as_string(df):\n    integer_string_features = []\n    code = ''; s = ''\n    try:\n        for col in df.columns:\n            print(\"-----------------------*****************----------------------------------\")\n            print(type(df[col][1]))\n            if df[col].dtype == 'object':\n                print(\"-----------------------*****************----------------------------------\")\n                print(df[col][1])\n                # Check each cell in the column for quoted integers\n                def check_cell(cell):\n                    if isinstance(cell, str):\n                        return bool(re.match('^\\d+$', cell))\n                    return False\n                \n                if df[col].apply(check_cell).any():\n                    integer_string_features.append(col)\n        \n        if len(integer_string_features) > 0:\n            global integer_as_string_present\n            integer_as_string_present = True\n            s = \"There are features with integers stored as strings in the dataset.\\n\"\n            # ...existing code...\n            s += \"Affected features: \" + str(integer_string_features[:len(integer_string_features)])\n            if len(integer_string_features) > 8:\n                s += \"...\\n\"\n            else:\n                s += \"\\n\"\n            s += f' Code for refactoring: \\n'\n            code += f'''\n    # Convert string integers to proper integer type\n    for col in df.columns:\n        if df[col].dtype == 'object':\n            if df[col].dropna().str.match('^\\d+$').all():\n                df[col] = pd.to_numeric(df[col], errors='coerce')\n    '''\n        else:\n            s = \"No integers stored as strings detected in the dataset.\\n\"\n        return s, code\n    except Exception as e:\n        return f\"Error in detect_integer_as_string: {str(e)}\\n\", \"\"\n\nspecial_char_present = False\n# @Description: To check for special characters in the dataset\ndef detect_special_characters(df):\n    special_char_features = []\n    code = ''; s = ''\n    try :\n        for col in df.columns:\n            if df[col].dtype == 'object':\n                pattern = re.compile('[^A-Za-z0-9\\s]+')\n                match = pattern.search(df[col].iloc[0])\n                if match:\n                    special_char_features.append(col)\n        if len(special_char_features) > 0:\n            global special_char_present; special_char_present = True\n            s = \"There are features with special characters in the dataset.\\n\"\n            s += \"Features with special characters: \" + str(special_char_features[:8] )\n            if len(special_char_features) > 8:\n                s += \"...\\n\"\n            else:\n                s += \"\\n\"\n            s+= f' Code for refactoring: \\n'\n            code += f'''\n    for col in df.columns:\n        if df[col].dtype == 'object':\n            pattern = re.compile('[^A-Za-z0-9\\s]+')\n            match = pattern.search(df[col].iloc[0])\n            if match:\n                # remove special characters\n                df[col] = df[col].str.replace('[^A-Za-z0-9\\s]+', '')\n                # remove leading and trailing spaces\n                df[col] = df[col].str.strip()\n                # convert to lowercase\n                df[col] = df[col].str.lower()\n                print(df[col].head())\\n'''\n        else:\n            s = \"There are no features with special characters in the dataset.\"\n    except Exception as e:\n        s = \"There are no features with special characters in the dataset.\"\n        # s = \"Nil.\"\n        # print(e)\n    return s, code\n\n\n# @Description: This function generates a bargraph for the number of special characters in each feature.\ndef generate_bargraph_special_characters(df):\n    if not special_char_present:\n        return None\n    try:\n        pres = {num : 0 for num in df.columns if df[num].str.contains('[^A-Za-z0-9\\s]+').sum()}\n        for num in df.columns:\n            if df[num].str.contains('[^A-Za-z0-9\\s]+').sum():\n                pres[num] = df[num].str.contains('[^A-Za-z0-9\\s]+').sum()\n        plt.bar(pres.keys(), pres.values())\n        plt.xticks(rotation=90)\n        # Save the plot to a BytesIO object\n        img_bytes = io.BytesIO()\n        plt.savefig(img_bytes, format='png')\n        img_bytes.seek(0)\n        # Encode the image data as base64 string\n        img_base64 = base64.b64encode(img_bytes.read()).decode('utf-8')\n        plt.close()\n\n        return img_base64\n    \n    except Exception as e:\n        # print(e)\n        return None\n    \n\ndef refactor_special_char(df):\n    try:\n        for col in df.columns:\n            if df[col].dtype == 'object':\n                pattern = re.compile('[^A-Za-z0-9\\s]+')\n                match = pattern.search(df[col].iloc[0])\n                if match:\n                    # Remove special characters\n                    df[col] = df[col].str.replace('[^A-Za-z0-9\\s]+', '', regex=True)\n                    # Remove leading and trailing spaces\n                    df[col] = df[col].str.strip()\n                    # Convert to lowercase\n                    df[col] = df[col].str.lower()\n        return df\n    except Exception as e:\n        return df\n\n\ndef refactor_integer_as_string(df):\n    try:\n        for col in df.columns:\n            if df[col].dtype == 'object':\n                def convert_cell(cell):\n                    if isinstance(cell, str):\n                        # Check if string contains only digits\n                        if cell.strip().isdigit():\n                            return int(cell)\n                    return cell\n                \n                # Apply conversion to each cell individually\n                df[col] = df[col].apply(convert_cell)\n        return df\n    except Exception as e:\n        print(f\"Error in refactor_integer_as_string: {str(e)}\")\n        return df\n# Detecting Trailng Spaces\n'''# create a list to store the column names with trailing spaces\ncols_with_trailing_spaces = []\n\n# loop through each column in the dataset\nfor col in df.columns:\n    # check if the column is a string type\n    if df[col].dtype == 'object':\n        # check if the column contains trailing spaces\n        if df[col].str.endswith(' ').any():\n            cols_with_trailing_spaces.append(col)\n\n# print the columns with trailing spaces\nif len(cols_with_trailing_spaces) > 0:\n    print(\"There are columns with trailing spaces in the dataset.\")\n    print(\"Columns with trailing spaces:\", cols_with_trailing_spaces)\nelse:\n    print(\"There are no columns with trailing spaces in the dataset.\")\n\n    \n# Check for different string interpretations due to capital letters usage\nfor col in df.select_dtypes(include=['object']):\n    unique_vals = df[col].str.lower().unique()\n    if len(unique_vals) != len(set(unique_vals)):\n        print(f\"Column '{col}' contains different string interpretations due to capital letters usage\")'''\n\ntrailing_spaces_present = False\n# @Description: This function detects trailing spaces in the dataset.\ndef trailing_spaces(df):\n    code = ''; s = ''\n    try:\n        cols_with_trailing_spaces = []\n        for col in df.columns:\n            if df[col].dtype == 'object':\n                if df[col].str.endswith(' ').any():\n                    cols_with_trailing_spaces.append(col)\n        if len(cols_with_trailing_spaces) > 0:\n            global trailing_spaces_present; trailing_spaces_present = True\n            s = \"There are columns with trailing spaces in the dataset.\"\n            s += \"Columns with trailing spaces: \" + str(cols_with_trailing_spaces[:8])\n            if len(cols_with_trailing_spaces) > 8:\n                s += \"...\\n\"\n            s+= f' Code for refactoring: \\n'\n            code += f'''\n    # # refactoring: \n    # for col in df.select_dtypes(include=['object']):\n    #     df[col] = df[col].str.strip()'''\n        else:\n            s = \"There are no columns with trailing spaces in the dataset.\"\n    except Exception as e:\n        s = \"There are no columns with trailing spaces in the dataset.\"\n        # s = \"Nil.\"\n        # print(e)\n    return s, code\n\ndef refactor_trailing_spaces(df):\n    try:\n        # Loop through each column and remove trailing spaces from string columns\n        for col in df.columns:\n            if df[col].dtype == 'object':  # Only consider object (string) columns\n                df[col] = df[col].str.strip()  # Remove leading and trailing spaces\n        return df\n    except Exception as e:\n        # If any exception occurs, return the original dataframe\n        return df\n\n\n# @Description: This function generates a bargraph for the number of trailing spaces in each feature.\ndef generate_bargraph_trailing_spaces(df):\n    global trailing_spaces_present\n    if not trailing_spaces_present:\n        return None\n    try: \n        pres = {num : 0 for num in df.columns if df[num].str.endswith(' ').sum()}\n        for num in df.columns:\n            if df[num].str.endswith(' ').sum():\n                pres[num] = df[num].str.endswith(' ').sum()\n        plt.bar(pres.keys(), pres.values())\n        plt.xticks(rotation=90)\n        # Save the plot to a BytesIO object\n        img_bytes = io.BytesIO()\n        plt.savefig(img_bytes, format='png')\n        img_bytes.seek(0)\n        # Encode the image data as base64 string\n        img_base64 = base64.b64encode(img_bytes.read()).decode('utf-8')\n        plt.close()\n\n        return img_base64\n    except Exception as e:\n        return None\n\n# Strings in Human Friendly Format\n\nhuman_friendly_present = False\ndef human_friendly(df):\n    code = ''; s = ''\n\n    pattern = r'^\\d{1,3}(,\\d{3})*(\\.\\d+)?$'\n    hum = False\n    for col in df.columns:\n        if df[col].dtype == 'object':\n            if df[col].str.match(pattern).all():\n                hum = True\n    if hum:\n        global human_friendly_present; human_friendly_present = True\n        s = \"There are human-friendly formats in the dataset.\"\n        s+= f' Code for refactoring: \\n'\n        code += f'''\n# # refactoring:\nfor col in df.columns:\n    if df[col].dtype == 'object':\n        if df[col].str.match(pattern).all():\n            # detected human-friendly format\n            # Convert human-friendly format to float\n            df[col] = df[col].str.replace(',', '').astype(float)'''\n    else:\n        s = \"There are no human-friendly formats in the dataset.\"\n    return s, code\n\ndef refactor_human_friendly(df):\n    try:\n        pattern = r'^\\d{1,3}(,\\d{3})*(\\.\\d+)?$'\n        for col in df.columns:\n            if df[col].dtype == 'object':\n                if df[col].str.match(pattern).all():\n                    # Remove commas and convert to float\n                    df[col] = df[col].str.replace(',', '').astype(float)\n        return df\n    except Exception as e:\n        return df\n\ndef generate_bargraph_human_friendly(df):\n    if not human_friendly_present:\n        return None\n    pattern = r'^\\d{1,3}(,\\d{3})*(\\.\\d+)?$'\n    pres = {num : 0 for num in df.columns if df[num].str.match(pattern).all()}\n    for num in df.columns:\n        if df[num].str.match(pattern).all():\n            pres[num] = df[num].str.match(pattern).all().sum()\n    plt.bar(pres.keys(), pres.values())\n    plt.xticks(rotation=90)\n    # Save the plot to a BytesIO object\n    img_bytes = io.BytesIO()\n    plt.savefig(img_bytes, format='png')\n    img_bytes.seek(0)\n    # Encode the image data as base64 string\n    img_base64 = base64.b64encode(img_bytes.read()).decode('utf-8')\n    plt.close()\n\n    return img_base64\n\n\n"
        },
        "ui.py": {
            "smells": [],
            "metrics": {
                "total_lines": 1,
                "num_functions": 0,
                "function_lengths": [],
                "num_prints": 0
            },
            "code": "# For testing certain things"
        }
    },
    "javascript": {
        "App.js": {
            "smells": [
                "Magic Numbers Found: 3 occurrences"
            ],
            "metrics": {
                "total_lines": 25,
                "num_console_logs": 0,
                "num_functions": 1,
                "function_lengths": [
                    9
                ],
                "max_nesting_level": 1
            },
            "code": "/**\n * @Date:  2023-02-28\n * @Language: React.js\n * @Description: App.js- This file is the key component of the frontend. It contains the navigation bar and the main page.\n */\n\nimport React from \"react\";\n// import { useState } from \"react\";\n// import axios from \"axios\";\nimport { RegExForm } from \"./components/RegularExpression/form\";\nimport MainPage from \"./components/DefaultPage/Mainpage\";\nimport NavigationBar from \"./components/Navbar/Navigationbar\";\n\n// @Description: This function returns the key components of the frontend.\nfunction App() {\n  return (\n    <div className=\"total-main\">\n      <NavigationBar />\n      <MainPage />\n      {/* <RegExForm/> */}\n    </div>\n  );\n}\n\nexport default App;\n"
        },
        "App.test.js": {
            "smells": [
                "Low Comment Density (<2%): 0 comments"
            ],
            "metrics": {
                "total_lines": 8,
                "num_console_logs": 0,
                "num_functions": 0,
                "function_lengths": [],
                "max_nesting_level": 1
            },
            "code": "import { render, screen } from '@testing-library/react';\nimport App from './App';\n\ntest('renders learn react link', () => {\n  render(<App />);\n  const linkElement = screen.getByText(/learn react/i);\n  expect(linkElement).toBeInTheDocument();\n});\n"
        },
        "index.js": {
            "smells": [],
            "metrics": {
                "total_lines": 17,
                "num_console_logs": 1,
                "num_functions": 0,
                "function_lengths": [],
                "max_nesting_level": 0
            },
            "code": "import React from 'react';\nimport ReactDOM from 'react-dom/client';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\n\nconst root = ReactDOM.createRoot(document.getElementById('root'));\nroot.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n"
        },
        "reportWebVitals.js": {
            "smells": [
                "Low Comment Density (<2%): 0 comments"
            ],
            "metrics": {
                "total_lines": 13,
                "num_console_logs": 0,
                "num_functions": 0,
                "function_lengths": [],
                "max_nesting_level": 3
            },
            "code": "const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n"
        },
        "setupTests.js": {
            "smells": [],
            "metrics": {
                "total_lines": 5,
                "num_console_logs": 0,
                "num_functions": 0,
                "function_lengths": [],
                "max_nesting_level": 0
            },
            "code": "// jest-dom adds custom jest matchers for asserting on DOM nodes.\n// allows you to do things like:\n// expect(element).toHaveTextContent(/react/i)\n// learn more: https://github.com/testing-library/jest-dom\nimport '@testing-library/jest-dom';\n"
        },
        "Codebox.js": {
            "smells": [
                "Magic Numbers Found: 1 occurrences",
                "Duplicate Code Blocks: 1 blocks repeated",
                "Inconsistent Naming Found: Mixed camelCase and snake_case (1 snake, 10 camel)"
            ],
            "metrics": {
                "total_lines": 92,
                "num_console_logs": 0,
                "num_functions": 0,
                "function_lengths": [],
                "max_nesting_level": 3
            },
            "code": "import React from 'react';\nimport { useState } from 'react';\nimport PropTypes from 'prop-types';\nimport { FaClipboard } from 'react-icons/fa';\n\nconst Codebox = ({ language, code }) => {\n  const [isCopied, setIsCopied] = useState(false);\n\n  const handleCopyClick = () => {\n    navigator.clipboard.writeText(code);\n    setIsCopied(true);\n    setTimeout(() => {\n      setIsCopied(false);\n    }, 2000);\n  };\n\n  if (!code) {\n    return <span></span>;\n  }\n  return (\n    <>\n    <style jsx>{`\n        .codebox {\n            background-color: #2b2b2b;\n            border-radius: 8px;\n            padding: 1rem;\n            overflow: auto;\n          }\n        \n          .code-header {\n            display: flex;\n            justify-content: space-between;\n            align-items: center;\n            padding-bottom: 0.5rem;\n            border-bottom: 1px solid #ddd;\n            color: #ddd;\n            font-size: 1.2rem;\n          }\n        \n          .copy-btn {\n            cursor: pointer;\n            background-color: transparent;\n            border: none;\n            color: #ddd;\n            font-size: 1.2rem;\n          }\n        \n          .copy-btn:hover {\n            color: #fff;\n          }\n        \n          .code-body {\n            margin: 0;\n            font-size: 1rem;\n            color: #ddd;\n            font-family: monospace;\n          }\n\n        \n    `}</style>\n\n    <div className=\"codebox\">\n      <div className=\"code-header\">\n        <span>{language}</span>\n        <button className=\"copy-btn\" title=\"Copy to clipboard\" onClick={handleCopyClick}>\n          {isCopied ? 'Copied!' : (<><FaClipboard /> Copy</>)} \n        </button>\n      </div>\n      <div className=\"code-body\">\n        <pre>\n          <code>{code}</code>\n        </pre>\n      </div>\n    </div>\n    </>\n  );\n};\n\nCodebox.propTypes = {\n  language: PropTypes.string.isRequired,\n  code: PropTypes.string.isRequired,\n};\n\nexport default Codebox;\n\n// To use in App.js:\n// npm i react-icons\n// import Codebox from './Codebox';\n// let snippet = `def hello_world():\n//     print(\"Hello World\")`;\n//  const language = 'python';\n//  <Codebox language={language} code={snippet} />\n"
        },
        "data.js": {
            "smells": [
                "Low Comment Density (<2%): 0 comments",
                "Magic Numbers Found: 134 occurrences"
            ],
            "metrics": {
                "total_lines": 235,
                "num_console_logs": 0,
                "num_functions": 0,
                "function_lengths": [],
                "max_nesting_level": 1
            },
            "code": "Object.defineProperty(exports, \"__esModule\", { value: true });\nexports.defaultData = [\n  {\n    \"Customer Name\": \"Romona Heaslip\",\n    Model: \"Taurus\",\n    Color: \"Aquamarine\",\n    \"Payment Mode\": \"Debit Card\",\n    \"Delivery Date\": \"07/11/2015\",\n    Amount: \"8529.22\",\n  },\n  {\n    \"Customer Name\": \"Clare Batterton\",\n    Model: \"Sparrow\",\n    Color: \"Pink\",\n    \"Payment Mode\": \"Cash On Delivery\",\n    \"Delivery Date\": \"7/13/2016\",\n    Amount: \"17866.19\",\n  },\n  {\n    \"Customer Name\": \"Eamon Traise\",\n    Model: \"Grand Cherokee\",\n    Color: \"Blue\",\n    \"Payment Mode\": \"Net Banking\",\n    \"Delivery Date\": \"09/04/2015\",\n    Amount: \"13853.09\",\n  },\n  {\n    \"Customer Name\": \"Julius Gorner\",\n    Model: \"GTO\",\n    Color: \"Aquamarine\",\n    \"Payment Mode\": \"Credit Card\",\n    \"Delivery Date\": \"12/15/2017\",\n    Amount: \"2338.74\",\n  },\n  {\n    \"Customer Name\": \"Jenna Schoolfield\",\n    Model: \"LX\",\n    Color: \"Yellow\",\n    \"Payment Mode\": \"Credit Card\",\n    \"Delivery Date\": \"10/08/2014\",\n    Amount: \"9578.45\",\n  },\n  {\n    \"Customer Name\": \"Marylynne Harring\",\n    Model: \"Catera\",\n    Color: \"Green\",\n    \"Payment Mode\": \"Cash On Delivery\",\n    \"Delivery Date\": \"7/01/2017\",\n    Amount: \"19141.62\",\n  },\n  {\n    \"Customer Name\": \"Vilhelmina Leipelt\",\n    Model: \"7 Series\",\n    Color: \"Goldenrod\",\n    \"Payment Mode\": \"Credit Card\",\n    \"Delivery Date\": \"12/20/2015\",\n    Amount: \"6543.30\",\n  },\n  {\n    \"Customer Name\": \"Barby Heisler\",\n    Model: \"Corvette\",\n    Color: \"Red\",\n    \"Payment Mode\": \"Credit Card\",\n    \"Delivery Date\": \"11/24/2014\",\n    Amount: \"13035.06\",\n  },\n  {\n    \"Customer Name\": \"Karyn Boik\",\n    Model: \"Regal\",\n    Color: \"Indigo\",\n    \"Payment Mode\": \"Debit Card\",\n    \"Delivery Date\": \"05/12/2014\",\n    Amount: \"18488.80\",\n  },\n  {\n    \"Customer Name\": \"Jeanette Pamplin\",\n    Model: \"S4\",\n    Color: \"Fuscia\",\n    \"Payment Mode\": \"Net Banking\",\n    \"Delivery Date\": \"12/30/2014\",\n    Amount: \"12317.04\",\n  },\n  {\n    \"Customer Name\": \"Cristi Espinos\",\n    Model: \"TL\",\n    Color: \"Aquamarine\",\n    \"Payment Mode\": \"Credit Card\",\n    \"Delivery Date\": \"12/18/2013\",\n    Amount: \"6230.13\",\n  },\n  {\n    \"Customer Name\": \"Issy Humm\",\n    Model: \"Club Wagon\",\n    Color: \"Pink\",\n    \"Payment Mode\": \"Cash On Delivery\",\n    \"Delivery Date\": \"02/02/2015\",\n    Amount: \"9709.49\",\n  },\n  {\n    \"Customer Name\": \"Tuesday Fautly\",\n    Model: \"V8 Vantage\",\n    Color: \"Crimson\",\n    \"Payment Mode\": \"Debit Card\",\n    \"Delivery Date\": \"11/19/2014\",\n    Amount: \"9766.10\",\n  },\n  {\n    \"Customer Name\": \"Rosemaria Thomann\",\n    Model: \"Caravan\",\n    Color: \"Violet\",\n    \"Payment Mode\": \"Net Banking\",\n    \"Delivery Date\": \"02/08/2014\",\n    Amount: \"7685.49\",\n  },\n  {\n    \"Customer Name\": \"Lyell Fuentez\",\n    Model: \"Bravada\",\n    Color: \"Violet\",\n    \"Payment Mode\": \"Debit Card\",\n    \"Delivery Date\": \"08/05/2016\",\n    Amount: \"18012.45\",\n  },\n  {\n    \"Customer Name\": \"Raynell Layne\",\n    Model: \"Colorado\",\n    Color: \"Pink\",\n    \"Payment Mode\": \"Credit Card\",\n    \"Delivery Date\": \"05/30/2016\",\n    Amount: \"2785.49\",\n  },\n  {\n    \"Customer Name\": \"Raye Whines\",\n    Model: \"4Runner\",\n    Color: \"Red\",\n    \"Payment Mode\": \"Debit Card\",\n    \"Delivery Date\": \"12/10/2016\",\n    Amount: \"9967.74\",\n  },\n  {\n    \"Customer Name\": \"Virgina Aharoni\",\n    Model: \"TSX\",\n    Color: \"Pink\",\n    \"Payment Mode\": \"Cash On Delivery\",\n    \"Delivery Date\": \"10/23/2014\",\n    Amount: \"5584.33\",\n  },\n  {\n    \"Customer Name\": \"Peta Cheshir\",\n    Model: \"Pathfinder\",\n    Color: \"Red\",\n    \"Payment Mode\": \"Net Banking\",\n    \"Delivery Date\": \"12/24/2015\",\n    Amount: \"5286.53\",\n  },\n  {\n    \"Customer Name\": \"Jule Urion\",\n    Model: \"Charger\",\n    Color: \"Violet\",\n    \"Payment Mode\": \"Debit Card\",\n    \"Delivery Date\": \"11/20/2013\",\n    Amount: \"13511.91\",\n  },\n  {\n    \"Customer Name\": \"Lew Gilyatt\",\n    Model: \"Bonneville\",\n    Color: \"Crimson\",\n    \"Payment Mode\": \"Credit Card\",\n    \"Delivery Date\": \"11/19/2013\",\n    Amount: \"6498.19\",\n  },\n  {\n    \"Customer Name\": \"Jobey Fortun\",\n    Model: \"B-Series\",\n    Color: \"Blue\",\n    \"Payment Mode\": \"Net Banking\",\n    \"Delivery Date\": \"10/30/2014\",\n    Amount: \"10359.67\",\n  },\n  {\n    \"Customer Name\": \"Blondie Crump\",\n    Model: \"Voyager\",\n    Color: \"Turquoise\",\n    \"Payment Mode\": \"Credit Card\",\n    \"Delivery Date\": \"04/06/2018\",\n    Amount: \"8118.39\",\n  },\n  {\n    \"Customer Name\": \"Florentia Binns\",\n    Model: \"Grand Prix\",\n    Color: \"Orange\",\n    \"Payment Mode\": \"Cash On Delivery\",\n    \"Delivery Date\": \"10/13/2016\",\n    Amount: \"10204.37\",\n  },\n  {\n    \"Customer Name\": \"Jaquelin Galtone\",\n    Model: \"Sunbird\",\n    Color: \"Red\",\n    \"Payment Mode\": \"Net Banking\",\n    \"Delivery Date\": \"10/22/2013\",\n    Amount: \"6528.06\",\n  },\n  {\n    \"Customer Name\": \"Hakeem Easseby\",\n    Model: \"Mirage\",\n    Color: \"Crimson\",\n    \"Payment Mode\": \"Debit Card\",\n    \"Delivery Date\": \"9/12/2014\",\n    Amount: \"5619.25\",\n  },\n  {\n    \"Customer Name\": \"Nickolaus Gidman\",\n    Model: \"XK\",\n    Color: \"Orange\",\n    \"Payment Mode\": \"Debit Card\",\n    \"Delivery Date\": \"05/12/2016\",\n    Amount: \"5091.43\",\n  },\n  {\n    \"Customer Name\": \"Jenine Iglesia\",\n    Model: \"Accord\",\n    Color: \"Orange\",\n    \"Payment Mode\": \"Debit Card\",\n    \"Delivery Date\": \"09/03/2018\",\n    Amount: \"14566.08\",\n  },\n  {\n    \"Customer Name\": \"Fax Witherspoon\",\n    Model: \"Range Rover Sport\",\n    Color: \"Orange\",\n    \"Payment Mode\": \"Credit Card\",\n    \"Delivery Date\": \"2/22/2018\",\n    Amount: \"5284.87\",\n  },\n];\n"
        },
        "Excel.js": {
            "smells": [
                "Magic Numbers Found: 8 occurrences"
            ],
            "metrics": {
                "total_lines": 50,
                "num_console_logs": 0,
                "num_functions": 1,
                "function_lengths": [
                    28
                ],
                "max_nesting_level": 1
            },
            "code": "import \"./excel.css\";\nimport {\n  RangeDirective,\n  RangesDirective,\n  SheetDirective,\n  SheetsDirective,\n  SpreadsheetComponent,\n  ColumnDirective,\n  ColumnsDirective,\n} from \"@syncfusion/ej2-react-spreadsheet\";\n\n\n\n// Registering Syncfusion license key\nimport { registerLicense } from \"@syncfusion/ej2-base\";\n\nregisterLicense(\n  \"Mgo+DSMBaFt+QHFqVkNrWU5FckBAXWFKblJ8RWZTelpgBShNYlxTR3ZbQ1pjSHpadkdiWn5X;Mgo+DSMBPh8sVXJ1S0d+X1RPc0BDWXxLflF1VWJTf1t6cVFWESFaRnZdQV1nSHlTd0FqWX1fdXRc;ORg4AjUWIQA/Gnt2VFhhQlJBfVpdXGdWfFN0RnNYdV51flBCcC0sT3RfQF5jTX9UdkRnUH9ccXNUTw==;MTc0MzQ4OEAzMjMxMmUzMTJlMzMzNUFvbDJNM3J4cExNckl0akFzNHBEM1dEWkNpTWRQWHUvL2l6VitudllBbDA9;MTc0MzQ4OUAzMjMxMmUzMTJlMzMzNVF0L3dJZlRkUW0rVWJFdTc4SGNzRFpmMW1GMnJ5U2o3dnN6YmMvRit2Zmc9;NRAiBiAaIQQuGjN/V0d+XU9Hc1RHQmJNYVF2R2BJflRwcV9DZEwgOX1dQl9gSXpScUViXXZecnZWRmk=;MTc0MzQ5MUAzMjMxMmUzMTJlMzMzNUhkTFIrMnVIemtyb0ErWmUzYU5yK1JkUVZ6N0o0NSs1Mm90Q3pJNXNUUE09;MTc0MzQ5MkAzMjMxMmUzMTJlMzMzNURtWEVuZGZLV21LM2JBVys4am1HNXV0UDlnaUVSVHF4SWw2MWlnRzY0SGs9;Mgo+DSMBMAY9C3t2VFhhQlJBfVpdXGdWfFN0RnNYdV51flBCcC0sT3RfQF5jTX9UdkRnUH9ccn1WTw==;MTc0MzQ5NEAzMjMxMmUzMTJlMzMzNWJETDIwZmV3TUtxckQ2WFJGZjdwSFVIdkhhMGkwdFRqQW85Rm5LeEhER2M9;MTc0MzQ5NUAzMjMxMmUzMTJlMzMzNWYvQ08zakxEUkZCTUd2dGtEZmMxcTFudXFnRityWVhsZjl4YzkzYUc4bWs9;MTc0MzQ5NkAzMjMxMmUzMTJlMzMzNUhkTFIrMnVIemtyb0ErWmUzYU5yK1JkUVZ6N0o0NSs1Mm90Q3pJNXNUUE09\"\n);\n\nfunction Excel(props) {\n\n  return (\n    <div className=\"Sheet\">\n      <SpreadsheetComponent\n        allowOpen={true}\n        openUrl=\"https://ej2services.syncfusion.com/production/web-services/api/spreadsheet/open\"\n        saveUrl=\"https://ej2services.syncfusion.com/production/web-services/api/spreadsheet/save\"\n      >\n        <SheetsDirective>\n          <SheetDirective>\n            <RangesDirective>\n              <RangeDirective dataSource={props.myjson}></RangeDirective>\n            </RangesDirective>\n            <ColumnsDirective>\n              <ColumnDirective width={160}></ColumnDirective>\n              <ColumnDirective width={130}></ColumnDirective>\n              <ColumnDirective width={130}></ColumnDirective>\n              <ColumnDirective width={130}></ColumnDirective>\n              <ColumnDirective width={120}></ColumnDirective>\n              <ColumnDirective width={120}></ColumnDirective>\n            </ColumnsDirective>\n          </SheetDirective>\n        </SheetsDirective>\n      </SpreadsheetComponent>\n    </div>\n  );\n}\n\nexport default Excel;\n"
        },
        "form.js": {
            "smells": [
                "Magic Numbers Found: 3 occurrences",
                "Duplicate Code Blocks: 2 blocks repeated",
                "Unused Variables: 2 variables",
                "Global Variables Found: 2 variables",
                "Long Chained Calls Found: 1 chains"
            ],
            "metrics": {
                "total_lines": 90,
                "num_console_logs": 2,
                "num_functions": 1,
                "function_lengths": [
                    4
                ],
                "max_nesting_level": 3
            },
            "code": "import React, { useState } from \"react\";\nimport axios from \"axios\";\nimport { Button } from \"react-bootstrap\";\nimport \"./form.css\";\n\nexport const RegExForm = () => {\n  const [regex, setRegex] = useState(\"\");\n  const [colNo, setColNo] = useState(0);\n  const [result, setResult] = useState(\"\");\n  const [count, setCount] = useState(null);\n  const [checked, setChecked] = useState(false);\n\n  const handleCheckboxChange = (event) => {\n    setChecked(event.target.checked);\n  };\n  const handleSubmit = (e) => {\n    e.preventDefault();\n    axios\n      .post(\"http://127.0.0.1:5000/regularExp\", {\n        regex: regex,\n        colNo: colNo,\n      })\n      .then((res) => {\n        //console.log(res);\n        setCount(res.data.count);\n        setResult(res.data.percent);\n        console.log(res.data);\n      });\n  };\n  const handleChangeRegex = (e) => {\n    setRegex(e.target.value);\n  };\n\n  const handleChangeColNum = (e) => {\n    setColNo(e.target.value);\n  };\n  return (\n    <div className=\"regex-container\">\n      <h1>Regular Expression Form</h1>\n      <div className=\"form-container\">\n        <form className=\"form-box\" onClick={handleSubmit}>\n          <div className=\"reg-exp\">\n            <label>Enter the RegEx for column: &nbsp; </label>\n            <input\n              type=\"text\"\n              name=\"regexrow\"\n              className=\"field\"\n              onChange={handleChangeRegex}\n            />\n          </div>\n          <div className=\"reg-exp\">\n            <label>Enter the Column number: &nbsp; &nbsp;</label>\n            <input\n              type=\"text\"\n              name=\"regexrow\"\n              className=\"field\"\n              onChange={handleChangeColNum}\n            />\n          </div>\n          <div className=\"d-flex justify-content-center my-2\">\n            <button\n              type=\"submit\"\n              // className=\"regex-btn align-items-center\"\n              // variant=\"primary\"\n            >\n              Submit\n            </button>\n          </div>\n          {/* {result ? <div>{result}</div> : null} */}\n          {result ? (\n            <div className=\"regex-results\">\n              <p>\n                {/* split result */}\n                {splitIntoSentences(result).map((sentence) => (\n                  <div>{sentence}</div>\n                ))}\n              </p>\n              {count ? <p>Number Of Matches:{count}</p> : null}\n            </div>\n          ) : null}\n        </form>\n      </div>\n    </div>\n  );\n};\n\nfunction splitIntoSentences(text) {\n  const sentences = text.split(\"\\n\");\n  return sentences.filter((sentence) => sentence.length > 0);\n}\n"
        }
    }
}